I"X!<!-- Style for Projects Page -->
<style>
	table {
		width:100%;
		background: none;
	}
	td.paper_text {
		/* padding-top: 1rem; */
		padding-bottom: 4rem;
		width: 70%;
	}
	p.summary {
		margin-top: 0.5rem;
		margin-bottom: 0rem;
	}
	.award {
		color: deeppink;
	}
	img.icon {
		padding-top: 0.3rem;
		border-radius: 10px;
		width: 100%;
	}
	@media screen and (max-width: 1200px) {
		td.paper_text {
			width: 70%;
		}
	}
</style>

<p></p>

<div>
  <table>
	<tbody>
		<!-- White-Box Cartoonization -->
		<tr>
			<td>
	  			<img class="icon" src="../assets/imgs/Projects/White-Box-Cartoonization-1.jpg" onmouseover="this.src='../assets/imgs/Projects/White-Box-Cartoonization-2.jpg';" onmouseout="this.src='../assets/imgs/Projects/White-Box-Cartoonization-1.jpg';" />
			</td>
      		<td class="paper_text">
				<strong>
					<a href="https://github.com/krishnasrikard/White-Box-Cartoonization" style="font-size: 1.15em; color:#0073e6; font-weight:600">Impact of Backbone Models and Dataset on Cartoonization Performance</a>
				</strong>
				<br />
				<p></p>
				<ul>
					<li>
					Explored the <b>impact of backbone architecture namely VGG19 and ViT-B/16</b> in the cartoonization process particularly in <b>structural and content loss functions</b>.
                    </li>
                    <li>
					Creating and Understanding a cartoonization process to a <b>specific style to cartoon</b> rather than a collection of styles.
                    </li>
                    <li>
					Observed a <b>high color pallet and coarse surface</b> with a transformer backbone (ViT) rather than a CNN backbone (VGG19). We also observed an improvement in the cartoonization process as the size of the dataset increases particularly the ViT backbone.
                    </li>
				</ul>
			</td>
    	</tr>
		<!-- An Efficient Approach to Super-Resolution with Fine-Tuning Diffusion Models -->
		<tr>
			<td>
	  			<img class="icon" src="../assets/imgs/Projects/SR-DDPM.png" onmouseover="this.src='../assets/imgs/Projects/SR-DDPM.png';" onmouseout="this.src='../assets/imgs/Projects/SR-DDPM.png';" />
			</td>
      		<td class="paper_text">
				<strong>
					<a href="https://github.com/krishnasrikard/SR-DDPM" style="font-size: 1.15em; color:#0073e6; font-weight:600">An Efficient Approach to Super-Resolution with Fine-Tuning Diffusion Models</a>
				</strong>
				<br />
				<p></p>
				<ul>
					<li>
						Explored the potential of <b>pre-trained diffusion model</b> SR3, specifically <b>fine-tuning</b> and <b>zero-shot</b> approaches for the task of image super-resolution.
					</li>
					<li>
						Demonstrated the generalization ability of fine-tuning process of SR3. The fine-tuning process is evaluated with <b>limited time steps, iterations and data samples</b>.
					</li>
					<li>
						Evaluated the zero-shot approach of using <b>range-null space decomposition</b> for super-resolution using unconditional DDPM with using a conditional DDPM <b>SR3 trained from scratch</b>.
					</li>
				</ul>
			</td>
    	</tr>
		<!-- Optical Flow Less Video Frame Interpolation -->
		<tr>
			<td>
	  			<img class="icon" src="../assets/imgs/Projects/Optical-Flow-Less-Video-Frame-Interpolation.gif" onmouseover="this.src='../assets/imgs/Projects/Optical-Flow-Less-Video-Frame-Interpolation.gif';" onmouseout="this.src='../assets/imgs/Projects/Optical-Flow-Less-Video-Frame-Interpolation.gif';" />
			</td>
      		<td class="paper_text">
				<strong>
					<a href="https://github.com/krishnasrikard/Optical-Flow-Less-Video-Frame-Interpolation" style="font-size: 1.15em; color:#0073e6; font-weight:600">Optical Flow Less Video Frame Interpolation</a>
				</strong>
				<br />
				<p></p>
				<ul>
					<li>
						Designed a <b>lightweight video restoration transformer</b> to capture long-range interactions, for fast inference and smaller training requirements for video frame interpolation. The model employs <b>self-attention</b> for feature extraction and <b>mutual-attention</b> as a surrogate to motion estimation to capture temporal information and feature alignment.
					</li>
					<li>
						Created a training procedure to predict intermediate frames of the video which are continuous with subsequent frames by <b>only looking at the previous frames</b> essentially following <b>causality</b>.
					</li>
					<li>
						Achieved <b>comparable results</b> with other SoTA video interpolation models.
					</li>
				</ul>
			</td>
    	</tr>
		<!-- Similarities between local-patch quality maps of NR IQA algorithms and saliency maps of computer vision classification models -->
		<tr>
			<td>
	  			<img class="icon" src="../assets/imgs/Projects/Saliency-Maps-NR-IQA-Classification-Models-SaliencyMap.png" onmouseover="this.src='../assets/imgs/Projects/Saliency-Maps-NR-IQA-Classification-Models-QualityMap.png';" onmouseout="this.src='../assets/imgs/Projects/Saliency-Maps-NR-IQA-Classification-Models-SaliencyMap.png';" />
			</td>
      		<td class="paper_text">
				<strong>
					<a href="https://github.com/krishnasrikard/Saliency-Maps-NR-IQA-Classification-Models" style="font-size: 1.15em; color:#0073e6; font-weight:600">Similarities between local-patch quality maps of NR IQA algorithms and saliency maps of computer vision classification models</a>
				</strong>
				<br />
				<p></p>
				<ul>
					<li>
						Achieved an understanding of similarities between <b>perception</b> of images by humans and classification models. NR-IQA models trained on human judgments/quality ratings are used to replicate the perception of humans. <b>Local-patch quality maps</b> provide the key areas focused on while rating an image.
					</li>
					<li>
						Using <b>PaQ-2-PiQ</b> to create local-patch quality maps for images. <b>ResNet18</b> is trained on images rated as good-quality images by PaQ-2-PiQ and saliency maps are generated <b>using Grad-CAM</b>.
					</li>
					<li>
						Compared the variation in <b>local-patch quality maps</b> and <b>saliency maps</b> due natural scene <b>distortions</b> like brightness, contrast, jpeg-compression, motion-blur, zoom-blur, etc. 
					</li>
				</ul>
			</td>
    	</tr>
		<!-- Reinforcement Learning for Autonomous Navigation of Cars -->
    	<tr>
			<td>
	  			<img class="icon" src="../assets/imgs/Projects/Autonomous-Driving-Segmentation.png" onmouseover="this.src='../assets/imgs/Projects/Autonomous-Driving-LaneDetection.png';" onmouseout="this.src='../assets/imgs/Projects/Autonomous-Driving-Segmentation.png';" />
			</td>
      		<td class="paper_text">
				<strong>
					<a href="https://github.com/krishnasrikard/Autonomous-Driving" style="font-size: 1.15em; color:#0073e6; font-weight:600">Reinforcement Learning for Autonomous Navigation of Cars</a>
				</strong>
				<br />
				<p></p>
				<ul>
					<li>
						Applied reinforcement learning for <b>autonomous navigation</b> of cars with two objectives; the car stays in the lane and the speed of the car is under the speed limit. Understood the benefits and difficulties of using reinforcement learning techniques for autonomous navigation.
					</li>
					<li>
						Designed the reward functions based on visual inputs from the camera using segmentation and lane detection models. The agent can regulate the speed and steering angle of the car and is trained using <b>Deep-Q-Learning</b>.
					</li>
				</ul>
			</td>
    	</tr>
		<!-- Effects of reduced frame corruptions on video classification -->
		<tr>
			<td>
	  			<img class="icon" src="../assets/imgs/Projects/Effects-of-reduced-frame-corruptions-on-video-classification.png" onmouseover="this.src='../assets/imgs/Projects/Effects-of-reduced-frame-corruptions-on-video-classification.png';" onmouseout="this.src='../assets/imgs/Projects/Effects-of-reduced-frame-corruptions-on-video-classification.png';" />
			</td>
      		<td class="paper_text">
				<strong>
					<a href="https://github.com/krishnasrikard/Effects-of-reduced-frame-corruptions-on-video-classification" style="font-size: 1.15em; color:#0073e6; font-weight:600">Effects of reduced frame corruptions on video classification</a>
				</strong>
				<br />
				<p></p>
				<ul>
					<li>
						Used CNN-RNN architecture for classifying videos.
					</li>
					<li>
						Designed various natural and adversarial single frame corruptions and understanding their impacts on classification.
					</li>
					<li>
						Designed a <b>reduced frame-level adversarial attack</b> to fool the video classification model.
					</li>
				</ul>
			</td>
    	</tr>
	</tbody>
</table>
</div>
:ET